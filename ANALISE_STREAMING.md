# AnÃ¡lise: ImplementaÃ§Ã£o de Streaming para GeraÃ§Ã£o de ContestaÃ§Ã£o

## ğŸ“‹ VisÃ£o Geral

Este documento detalha as mudanÃ§as necessÃ¡rias para implementar **streaming visÃ­vel** na geraÃ§Ã£o de contestaÃ§Ãµes, permitindo que o usuÃ¡rio veja o texto sendo gerado em tempo real, ao invÃ©s de esperar pela resposta completa.

---

## ğŸ” Estado Atual

### Backend (`modules/llm_generator.py`)

**LocalizaÃ§Ã£o**: Linhas 138-147

```python
response = self.client.messages.create(
    model=Config.CLAUDE_MODEL,
    max_tokens=max_tokens,
    temperature=temperatura,
    top_k=top_k,
    system=SYSTEM_PROMPT,
    messages=[
        {"role": "user", "content": prompt_usuario}
    ]
)
```

**Problema**:
- Usa `messages.create()` que Ã© **sÃ­ncrono/bloqueante**
- Aguarda a resposta completa antes de retornar
- UsuÃ¡rio fica esperando sem feedback visual durante a geraÃ§Ã£o

### Frontend (`app.py`)

**LocalizaÃ§Ã£o**: Linhas 307-313

```python
st.text_area(
    "Texto da contestaÃ§Ã£o",
    value=res['contestacao'],
    height=500,
    label_visibility="collapsed"
)
```

**Problema**:
- Exibe apenas o resultado final
- Sem atualizaÃ§Ã£o em tempo real
- Sem feedback de progresso

---

## âœ… MudanÃ§as NecessÃ¡rias

### 1ï¸âƒ£ Backend: Implementar Streaming na API

#### Arquivo: `modules/llm_generator.py`

**Modificar o mÃ©todo `gerar_contestacao()` para suportar streaming:**

```python
def gerar_contestacao(
    self,
    dados_peticao: Dict,
    contexto_rag: Dict,
    temperatura: float = Config.DEFAULT_TEMPERATURE,
    top_k: int = Config.DEFAULT_TOP_K,
    max_tokens: int = Config.DEFAULT_MAX_TOKENS,
    stream: bool = True  # NOVO PARÃ‚METRO
) -> Dict:
    """
    Gera contestaÃ§Ã£o via Claude API com suporte a streaming

    Args:
        dados_peticao: Dados estruturados da petiÃ§Ã£o
        contexto_rag: Contexto RAG construÃ­do
        temperatura: ParÃ¢metro de temperatura (0.3-0.9)
        top_k: ParÃ¢metro top-k (20-60)
        max_tokens: Tokens mÃ¡ximos para geraÃ§Ã£o
        stream: Se True, retorna generator; se False, retorna texto completo

    Returns:
        Se stream=False: Dict com contestaÃ§Ã£o gerada e metadados
        Se stream=True: Generator que yielda chunks de texto
    """
    # ... cÃ³digo de validaÃ§Ã£o e preparaÃ§Ã£o ...

    # Construir prompts
    prompt_usuario = construir_prompt_usuario(dados_peticao, contexto_rag)

    if stream:
        # NOVO: Modo streaming
        return self._gerar_com_streaming(
            prompt_usuario=prompt_usuario,
            temperatura=temperatura,
            top_k=top_k,
            max_tokens=max_tokens,
            dados_peticao=dados_peticao
        )
    else:
        # Modo original (nÃ£o-streaming)
        # ... cÃ³digo atual ...
```

#### Adicionar novo mÃ©todo `_gerar_com_streaming()`:

```python
def _gerar_com_streaming(
    self,
    prompt_usuario: str,
    temperatura: float,
    top_k: int,
    max_tokens: int,
    dados_peticao: Dict
):
    """
    Generator que yielda chunks de texto em streaming

    Yields:
        Dict contendo:
        - 'chunk': Texto do chunk atual
        - 'done': Boolean indicando se terminou
        - 'metadata': Metadados (apenas no Ãºltimo chunk)
    """
    try:
        # Usar stream context manager
        with self.client.messages.stream(
            model=Config.CLAUDE_MODEL,
            max_tokens=max_tokens,
            temperature=temperatura,
            top_k=top_k,
            system=SYSTEM_PROMPT,
            messages=[
                {"role": "user", "content": prompt_usuario}
            ]
        ) as stream:
            # Iterar sobre os eventos de streaming
            for text in stream.text_stream:
                yield {
                    'chunk': text,
                    'done': False,
                    'metadata': None
                }

            # Ãšltimo chunk com metadados
            final_message = stream.get_final_message()

            metadados = {
                'model': Config.CLAUDE_MODEL,
                'temperatura': temperatura,
                'top_k': top_k,
                'input_tokens': final_message.usage.input_tokens,
                'output_tokens': final_message.usage.output_tokens,
                'stop_reason': final_message.stop_reason,
                'tipo_caso': dados_peticao.get('tipo_caso'),
                'confianca_classificacao': dados_peticao.get('confianca')
            }

            # Calcular custo
            custo_input = (metadados['input_tokens'] / 1_000_000) * 15
            custo_output = (metadados['output_tokens'] / 1_000_000) * 75
            custo_total = custo_input + custo_output

            yield {
                'chunk': '',
                'done': True,
                'metadata': {
                    **metadados,
                    'custo_estimado': custo_total
                }
            }

    except anthropic.APIError as e:
        yield {
            'chunk': '',
            'done': True,
            'error': str(e),
            'metadata': None
        }
    except Exception as e:
        yield {
            'chunk': '',
            'done': True,
            'error': str(e),
            'metadata': None
        }
```

---

### 2ï¸âƒ£ Frontend: Atualizar UI para Mostrar Streaming

#### Arquivo: `app.py`

**Modificar a seÃ§Ã£o de geraÃ§Ã£o (linhas ~210-240):**

```python
# Dentro do if uploaded_file:
if st.button("ğŸš€ Gerar ContestaÃ§Ã£o", type="primary", use_container_width=True):

    with st.spinner("Processando petiÃ§Ã£o inicial..."):
        try:
            # 1. Processar documento
            dados_peticao = st.session_state.processor.processar_documento(
                uploaded_file
            )
            st.session_state.dados_peticao = dados_peticao

            # 2. Buscar contexto RAG
            resultado_rag = st.session_state.rag.buscar_contexto(
                dados_peticao['fatos_completos']
            )

            # 3. Construir contexto otimizado
            contexto_rag = st.session_state.context_builder.construir_contexto(
                dados_peticao,
                resultado_rag
            )

            st.success("âœ… Processamento concluÃ­do!")

        except Exception as e:
            st.error(f"âŒ Erro no processamento: {str(e)}")
            st.stop()

    # 4. NOVO: Gerar contestaÃ§Ã£o com streaming
    st.divider()
    st.header("ğŸ“„ ContestaÃ§Ã£o em GeraÃ§Ã£o...")

    # Container para o texto streaming
    contestacao_container = st.empty()
    metrics_container = st.empty()

    texto_acumulado = ""
    metadados_finais = None

    try:
        # Obter generator de streaming
        stream_generator = st.session_state.llm_generator.gerar_contestacao(
            dados_peticao=dados_peticao,
            contexto_rag=contexto_rag,
            temperatura=temperatura,
            top_k=top_k,
            max_tokens=max_tokens,
            stream=True  # ATIVAR STREAMING
        )

        # Processar chunks em tempo real
        for chunk_data in stream_generator:
            if chunk_data.get('error'):
                st.error(f"âŒ Erro na geraÃ§Ã£o: {chunk_data['error']}")
                break

            if not chunk_data['done']:
                # Acumular texto
                texto_acumulado += chunk_data['chunk']

                # Atualizar display em tempo real
                with contestacao_container.container():
                    st.text_area(
                        "ContestaÃ§Ã£o sendo gerada...",
                        value=texto_acumulado,
                        height=500,
                        label_visibility="collapsed",
                        key=f"streaming_{len(texto_acumulado)}"
                    )
            else:
                # Ãšltimo chunk - salvar metadados
                metadados_finais = chunk_data.get('metadata')

        # 5. Validar qualidade
        st.info("ğŸ” Validando qualidade...")
        validacao = st.session_state.validator.validar_contestacao(
            texto_acumulado,
            dados_peticao
        )

        # Salvar resultado completo
        st.session_state.resultado = {
            'contestacao': texto_acumulado,
            'dados_peticao': dados_peticao,
            'contexto_rag': contexto_rag,
            'metadados': metadados_finais,
            'validacao': validacao,
            'custo': metadados_finais['custo_estimado']
        }

        # Mostrar mÃ©tricas finais
        with metrics_container.container():
            col1, col2, col3, col4 = st.columns(4)

            with col1:
                st.metric("Tipo de Caso", dados_peticao.get('tipo_caso', 'N/A'))

            with col2:
                conf = dados_peticao.get('confianca', 0)
                st.metric("ConfianÃ§a", f"{conf:.1%}")

            with col3:
                st.metric("Tokens", f"{metadados_finais['output_tokens']:,}")

            with col4:
                st.metric("Custo", f"${metadados_finais['custo_estimado']:.4f}")

        st.success("âœ… ContestaÃ§Ã£o gerada com sucesso!")

    except Exception as e:
        st.error(f"âŒ Erro: {str(e)}")
        import traceback
        st.code(traceback.format_exc())
```

---

## ğŸ¯ Alternativa: Usar `st.write_stream()`

Streamlit tem uma funÃ§Ã£o nativa `st.write_stream()` otimizada para streaming:

```python
# VersÃ£o simplificada usando st.write_stream()
contestacao_placeholder = st.empty()

def text_generator():
    """Adapter para st.write_stream()"""
    for chunk_data in stream_generator:
        if not chunk_data['done'] and not chunk_data.get('error'):
            yield chunk_data['chunk']

# Exibir streaming
with contestacao_placeholder.container():
    st.subheader("ğŸ“œ ContestaÃ§Ã£o")
    texto_acumulado = st.write_stream(text_generator())
```

---

## ğŸ“¦ DependÃªncias

### Verificar versÃ£o do pacote Anthropic

O streaming estÃ¡ disponÃ­vel a partir da versÃ£o `0.18.0`. Verifique `requirements.txt`:

```txt
anthropic>=0.25.0  # âœ… JÃ¡ suporta streaming
```

**Nenhuma mudanÃ§a necessÃ¡ria** - a versÃ£o atual jÃ¡ suporta streaming.

---

## ğŸ”§ ConfiguraÃ§Ãµes Adicionais

### OpÃ§Ã£o: Adicionar toggle de streaming no sidebar

```python
# No sidebar de parÃ¢metros
usar_streaming = st.sidebar.checkbox(
    "ğŸ”„ Streaming em Tempo Real",
    value=True,
    help="Mostra texto sendo gerado em tempo real"
)
```

---

## ğŸ“Š ComparaÃ§Ã£o: Antes vs Depois

### Antes (Sem Streaming)

```
[UsuÃ¡rio clica "Gerar"]
  â†“
[Spinner girando... 30-60 segundos]
  â†“
[Texto completo aparece de uma vez]
```

**ExperiÃªncia**:
- âŒ Sem feedback durante geraÃ§Ã£o
- âŒ Parece travado
- âŒ UsuÃ¡rio nÃ£o sabe o que estÃ¡ acontecendo

### Depois (Com Streaming)

```
[UsuÃ¡rio clica "Gerar"]
  â†“
[Texto comeÃ§a a aparecer palavra por palavra]
  â†“
[UsuÃ¡rio vÃª a contestaÃ§Ã£o sendo escrita em tempo real]
  â†“
[MÃ©tricas aparecem ao final]
```

**ExperiÃªncia**:
- âœ… Feedback imediato
- âœ… Engajamento visual
- âœ… TransparÃªncia do processo

---

## ğŸš€ ImplementaÃ§Ã£o Recomendada - Passo a Passo

### Etapa 1: Backend (30 min)
1. Adicionar parÃ¢metro `stream=True` ao mÃ©todo `gerar_contestacao()`
2. Implementar mÃ©todo `_gerar_com_streaming()`
3. Testar com script simples para verificar chunks

### Etapa 2: Frontend (45 min)
1. Modificar lÃ³gica do botÃ£o "Gerar ContestaÃ§Ã£o"
2. Adicionar containers para texto streaming
3. Implementar loop de acumulaÃ§Ã£o de chunks
4. Atualizar UI em tempo real

### Etapa 3: Testes (20 min)
1. Testar com petiÃ§Ã£o real
2. Verificar se mÃ©tricas aparecem corretamente
3. Validar tratamento de erros

### Etapa 4: Refinamentos (15 min)
1. Adicionar animaÃ§Ãµes/indicadores
2. Ajustar altura/layout dos containers
3. Adicionar toggle opcional no sidebar

**Tempo total estimado**: ~2 horas

---

## âš ï¸ ConsideraÃ§Ãµes Importantes

### 1. Performance de Rede
- Streaming funciona melhor com conexÃµes estÃ¡veis
- Em redes lentas, pode haver delay entre chunks

### 2. LimitaÃ§Ãµes do Streamlit
- `st.text_area()` com `key` dinÃ¢mica pode causar re-renderizaÃ§Ã£o
- Considere usar `st.markdown()` ou `st.write()` ao invÃ©s de `text_area()`

### 3. Estado da AplicaÃ§Ã£o
- Durante streaming, evitar que usuÃ¡rio clique novamente em "Gerar"
- Desabilitar botÃ£o ou usar `st.session_state` para controlar

### 4. Exemplo de ProteÃ§Ã£o Contra Duplo Clique

```python
# Adicionar flag de geraÃ§Ã£o em andamento
if 'gerando' not in st.session_state:
    st.session_state.gerando = False

# No botÃ£o
if st.button(
    "ğŸš€ Gerar ContestaÃ§Ã£o",
    disabled=st.session_state.gerando,
    type="primary"
):
    st.session_state.gerando = True
    try:
        # ... lÃ³gica de geraÃ§Ã£o ...
    finally:
        st.session_state.gerando = False
```

---

## ğŸ“ Resumo das MudanÃ§as

### Arquivos a Modificar

| Arquivo | MudanÃ§as | Complexidade |
|---------|----------|--------------|
| `modules/llm_generator.py` | Adicionar mÃ©todo streaming + parÃ¢metro stream | MÃ©dia |
| `app.py` | Modificar lÃ³gica de geraÃ§Ã£o e display | MÃ©dia |
| `config/settings.py` | (Opcional) Adicionar flag `ENABLE_STREAMING` | Baixa |

### Linhas de CÃ³digo

- **Backend**: ~60 linhas novas
- **Frontend**: ~40 linhas modificadas
- **Total**: ~100 linhas

---

## ğŸ¬ PrÃ³ximos Passos

1. âœ… Revisar esta anÃ¡lise
2. â¬œ Implementar backend streaming
3. â¬œ Implementar frontend streaming
4. â¬œ Testar end-to-end
5. â¬œ Commit e push das mudanÃ§as

---

## ğŸ“š ReferÃªncias

- [Anthropic Streaming Documentation](https://docs.anthropic.com/en/api/messages-streaming)
- [Streamlit Streaming Documentation](https://docs.streamlit.io/develop/api-reference/write-magic/st.write_stream)
- CÃ³digo atual: `modules/llm_generator.py:138-147`
- CÃ³digo atual: `app.py:245-313`

---

**Gerado em**: 2025-11-14
**VersÃ£o**: 1.0
