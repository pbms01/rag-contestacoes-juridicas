# üöÄ Guia de Implementa√ß√£o: Streaming para Gera√ß√£o de Contesta√ß√£o

## ‚úÖ Checklist de Implementa√ß√£o

### Fase 1: Prepara√ß√£o (5 min)

- [ ] Fazer backup dos arquivos atuais
- [ ] Ler documenta√ß√£o de refer√™ncia:
  - `ANALISE_STREAMING.md` - An√°lise completa
  - `EXEMPLO_llm_generator_streaming.py` - C√≥digo backend
  - `EXEMPLO_app_streaming.py` - C√≥digo frontend
- [ ] Criar branch para desenvolvimento
- [ ] Verificar vers√£o do pacote `anthropic` (>= 0.25.0)

### Fase 2: Backend (30-45 min)

- [ ] Abrir `modules/llm_generator.py`
- [ ] Adicionar par√¢metro `stream: bool = False` ao m√©todo `gerar_contestacao()`
- [ ] Renomear m√©todo atual para `_gerar_sem_streaming()`
- [ ] Adicionar novo m√©todo `_gerar_com_streaming()`
- [ ] Adicionar l√≥gica de escolha entre streaming/n√£o-streaming
- [ ] Testar backend isoladamente

### Fase 3: Frontend (30-45 min)

- [ ] Abrir `app.py`
- [ ] Localizar se√ß√£o de gera√ß√£o (linhas ~210-240)
- [ ] Adicionar containers para streaming (`st.empty()`)
- [ ] Modificar chamada para incluir `stream=True`
- [ ] Implementar loop de processamento de chunks
- [ ] Adicionar atualiza√ß√£o em tempo real do texto
- [ ] Testar frontend com backend

### Fase 4: Testes (20 min)

- [ ] Testar com peti√ß√£o simples
- [ ] Testar com peti√ß√£o complexa
- [ ] Verificar tratamento de erros
- [ ] Validar m√©tricas finais
- [ ] Testar regenera√ß√£o

### Fase 5: Refinamentos (15 min)

- [ ] Ajustar estilos CSS
- [ ] Adicionar cursor piscando
- [ ] Melhorar mensagens de status
- [ ] Adicionar toggle streaming no sidebar (opcional)
- [ ] Documentar mudan√ßas

### Fase 6: Finaliza√ß√£o (10 min)

- [ ] Commit das mudan√ßas
- [ ] Push para reposit√≥rio
- [ ] Atualizar documenta√ß√£o do projeto

---

## üìù Passo a Passo Detalhado

### PASSO 1: Backup

```bash
# Fazer backup dos arquivos que ser√£o modificados
cp modules/llm_generator.py modules/llm_generator.py.backup
cp app.py app.py.backup
```

### PASSO 2: Modificar Backend

#### 2.1: Abrir arquivo

```bash
# Abrir no editor de prefer√™ncia
code modules/llm_generator.py
```

#### 2.2: Modificar assinatura do m√©todo `gerar_contestacao()`

**Linha ~93** - Adicionar par√¢metro `stream`:

```python
def gerar_contestacao(
    self,
    dados_peticao: Dict,
    contexto_rag: Dict,
    temperatura: float = Config.DEFAULT_TEMPERATURE,
    top_k: int = Config.DEFAULT_TOP_K,
    max_tokens: int = Config.DEFAULT_MAX_TOKENS,
    stream: bool = False  # üî• NOVO PAR√ÇMETRO
) -> Union[Dict, Generator]:  # üî• MODIFICAR TIPO DE RETORNO
```

#### 2.3: Adicionar import

**Linha ~9** - Adicionar tipo Generator:

```python
from typing import Dict, List, Optional, Generator, Union
```

#### 2.4: Modificar corpo do m√©todo

**Linhas ~135-200** - Substituir por:

```python
# Construir prompts
prompt_usuario = construir_prompt_usuario(dados_peticao, contexto_rag)

# NOVA L√ìGICA: Escolher entre streaming ou n√£o
if stream:
    return self._gerar_com_streaming(
        prompt_usuario=prompt_usuario,
        temperatura=temperatura,
        top_k=top_k,
        max_tokens=max_tokens,
        dados_peticao=dados_peticao
    )
else:
    return self._gerar_sem_streaming(
        prompt_usuario=prompt_usuario,
        temperatura=temperatura,
        top_k=top_k,
        max_tokens=max_tokens,
        dados_peticao=dados_peticao
    )
```

#### 2.5: Renomear m√©todo original

Copiar c√≥digo atual das linhas **135-200** para novo m√©todo `_gerar_sem_streaming()`:

```python
def _gerar_sem_streaming(
    self,
    prompt_usuario: str,
    temperatura: float,
    top_k: int,
    max_tokens: int,
    dados_peticao: Dict
) -> Dict:
    """Gera contesta√ß√£o de forma tradicional (sem streaming)"""

    # ... c√≥digo original aqui ...
    response = self.client.messages.create(...)
    # ... resto do c√≥digo ...
```

#### 2.6: Adicionar m√©todo de streaming

**Ap√≥s `_gerar_sem_streaming()`**, adicionar:

```python
def _gerar_com_streaming(
    self,
    prompt_usuario: str,
    temperatura: float,
    top_k: int,
    max_tokens: int,
    dados_peticao: Dict
) -> Generator[Dict, None, None]:
    """Gera contesta√ß√£o com streaming"""

    try:
        with self.client.messages.stream(
            model=Config.CLAUDE_MODEL,
            max_tokens=max_tokens,
            temperature=temperatura,
            top_k=top_k,
            system=SYSTEM_PROMPT,
            messages=[{"role": "user", "content": prompt_usuario}]
        ) as stream:

            # Yield chunks de texto
            for text in stream.text_stream:
                yield {
                    'chunk': text,
                    'done': False,
                    'metadata': None,
                    'error': None
                }

            # Metadados finais
            final_message = stream.get_final_message()

            metadados = {
                'model': Config.CLAUDE_MODEL,
                'temperatura': temperatura,
                'top_k': top_k,
                'input_tokens': final_message.usage.input_tokens,
                'output_tokens': final_message.usage.output_tokens,
                'stop_reason': final_message.stop_reason,
                'tipo_caso': dados_peticao.get('tipo_caso'),
                'confianca_classificacao': dados_peticao.get('confianca')
            }

            # Calcular custo
            custo_input = (metadados['input_tokens'] / 1_000_000) * 15
            custo_output = (metadados['output_tokens'] / 1_000_000) * 75
            metadados['custo_estimado'] = custo_input + custo_output

            # √öltimo yield com metadados
            yield {
                'chunk': '',
                'done': True,
                'metadata': metadados,
                'error': None
            }

    except Exception as e:
        yield {
            'chunk': '',
            'done': True,
            'metadata': None,
            'error': str(e)
        }
```

### PASSO 3: Modificar Frontend

#### 3.1: Abrir arquivo

```bash
code app.py
```

#### 3.2: Localizar se√ß√£o de gera√ß√£o

**Linhas ~210-240** - Dentro do `if st.button("üöÄ Gerar Contesta√ß√£o"...)`

#### 3.3: Substituir c√≥digo ap√≥s processamento RAG

**Ap√≥s a linha ~240** (depois do processamento RAG), substituir por:

```python
st.success("‚úÖ Processamento conclu√≠do!")

# =========================================
# GERA√á√ÉO COM STREAMING
# =========================================
st.divider()
st.header("üìÑ Gera√ß√£o da Contesta√ß√£o")

# Containers
status_container = st.empty()
texto_container = st.empty()

# Vari√°veis
texto_acumulado = ""
metadados_finais = None

# Mostrar status
with status_container.container():
    st.info("üåä Gerando contesta√ß√£o em tempo real...")

# Obter generator
stream_generator = st.session_state.llm_generator.gerar_contestacao(
    dados_peticao=dados_peticao,
    contexto_rag=contexto_rag,
    temperatura=temperatura,
    top_k=top_k,
    max_tokens=max_tokens,
    stream=True  # üî• ATIVAR STREAMING
)

# Processar chunks
for chunk_data in stream_generator:

    # Verificar erro
    if chunk_data.get('error'):
        with status_container.container():
            st.error(f"‚ùå Erro: {chunk_data['error']}")
        break

    # Processar chunk
    if not chunk_data['done']:
        # Acumular texto
        texto_acumulado += chunk_data['chunk']

        # Atualizar display
        with texto_container.container():
            st.markdown("### üìú Contesta√ß√£o")
            st.text_area(
                "Texto em gera√ß√£o...",
                value=texto_acumulado,
                height=500,
                label_visibility="collapsed",
                key=f"stream_{len(texto_acumulado)}"  # Key √∫nica
            )
    else:
        # Salvar metadados
        metadados_finais = chunk_data.get('metadata')

# Atualizar status final
with status_container.container():
    st.success("‚úÖ Contesta√ß√£o gerada com sucesso!")

# Validar qualidade
with st.spinner("üîç Validando qualidade..."):
    validacao = st.session_state.validator.validar_contestacao(
        texto_acumulado,
        dados_peticao
    )

# Salvar resultado
st.session_state.resultado = {
    'contestacao': texto_acumulado,
    'dados_peticao': dados_peticao,
    'contexto_rag': contexto_rag,
    'metadados': metadados_finais,
    'validacao': validacao,
    'custo': metadados_finais['custo_estimado']
}

# Mostrar m√©tricas (c√≥digo existente pode ser mantido)
# ...
```

### PASSO 4: Testar

#### 4.1: Testar backend isoladamente

```python
# Criar arquivo de teste: test_streaming.py
from modules.llm_generator import LLMGenerator

# Dados de teste
dados = {'tipo_caso': 'Teste', 'confianca': 0.9}
contexto = {'nivel_1': [], 'nivel_2': [], 'nivel_3': []}

# Testar streaming
gen = LLMGenerator()
stream = gen.gerar_contestacao(dados, contexto, stream=True)

texto = ""
for chunk_data in stream:
    if not chunk_data['done']:
        texto += chunk_data['chunk']
        print(chunk_data['chunk'], end='', flush=True)
    else:
        print(f"\n\nMetadados: {chunk_data['metadata']}")

print(f"\n\nTexto completo: {len(texto)} caracteres")
```

#### 4.2: Testar frontend completo

```bash
streamlit run app.py
```

---

## üé® Melhorias Opcionais

### Op√ß√£o 1: Adicionar Cursor Piscando

```python
# No texto_container
st.markdown(f"""
<div style="...">
{texto_acumulado}<span style="animation: blink 1s infinite;">‚ñä</span>
</div>

<style>
@keyframes blink {{
    0%, 50% {{ opacity: 1; }}
    51%, 100% {{ opacity: 0; }}
}}
</style>
""", unsafe_allow_html=True)
```

### Op√ß√£o 2: Toggle no Sidebar

```python
# No sidebar
usar_streaming = st.sidebar.checkbox(
    "üåä Streaming em Tempo Real",
    value=True,
    help="Mostra texto sendo gerado ao vivo"
)

# Usar na chamada
stream_generator = st.session_state.llm_generator.gerar_contestacao(
    ...,
    stream=usar_streaming  # Usar toggle
)
```

### Op√ß√£o 3: Barra de Progresso

```python
# Antes do loop
progress_bar = st.progress(0)

# Dentro do loop
progresso = min(len(texto_acumulado) / 10000, 0.99)  # Estimativa
progress_bar.progress(progresso)
```

---

## ‚ö†Ô∏è Problemas Comuns

### Problema 1: Texto n√£o atualiza em tempo real

**Causa**: Key da text_area n√£o est√° mudando

**Solu√ß√£o**:
```python
st.text_area(
    ...,
    key=f"stream_{len(texto_acumulado)}"  # Key √∫nica a cada update
)
```

### Problema 2: Erro "Generator object is not iterable"

**Causa**: Esqueceu de usar `yield` no m√©todo streaming

**Solu√ß√£o**: Verificar que `_gerar_com_streaming()` usa `yield`, n√£o `return`

### Problema 3: Metadados None no final

**Causa**: N√£o processou o √∫ltimo chunk com `done=True`

**Solu√ß√£o**: Garantir que loop processa todos os chunks:
```python
for chunk_data in stream_generator:  # N√£o usar if/break prematuramente
    ...
```

### Problema 4: Display fica lento com texto grande

**Causa**: Re-renderizar text_area completa a cada chunk √© custoso

**Solu√ß√£o**: Usar `st.markdown()` ou limitar frequ√™ncia de updates:
```python
if len(texto_acumulado) % 100 == 0:  # Atualizar a cada 100 caracteres
    atualizar_display()
```

---

## üìä Valida√ß√£o

### Checklist de Testes

- [ ] Streaming funciona com peti√ß√£o simples
- [ ] Streaming funciona com peti√ß√£o complexa
- [ ] Texto acumula corretamente
- [ ] Metadados aparecem no final
- [ ] Custo √© calculado corretamente
- [ ] Erros s√£o tratados apropriadamente
- [ ] Valida√ß√£o de qualidade funciona ap√≥s streaming
- [ ] Bot√µes de a√ß√£o funcionam ap√≥s gera√ß√£o
- [ ] Download/c√≥pia funcionam com texto gerado
- [ ] Regenera√ß√£o funciona

### M√©tricas de Sucesso

- ‚úÖ Texto aparece em < 1 segundo ap√≥s iniciar
- ‚úÖ Chunks aparecem suavemente (sem travamentos)
- ‚úÖ Metadados corretos ao final
- ‚úÖ Sem erros no console
- ‚úÖ UX melhorou (feedback visual)

---

## üéØ Resumo

### O que muda:

1. **Backend**: M√©todo `gerar_contestacao()` agora suporta `stream=True`
2. **Frontend**: Loop processa chunks e atualiza UI em tempo real
3. **UX**: Usu√°rio v√™ texto sendo gerado ao inv√©s de spinner

### O que N√ÉO muda:

1. ‚úÖ Processamento da peti√ß√£o
2. ‚úÖ Busca RAG
3. ‚úÖ Valida√ß√£o de qualidade
4. ‚úÖ M√©tricas e estat√≠sticas
5. ‚úÖ Download e c√≥pia

### Compatibilidade:

- ‚úÖ Modo streaming (novo) - `stream=True`
- ‚úÖ Modo tradicional (original) - `stream=False` (padr√£o)

---

## üìö Pr√≥ximos Passos

Ap√≥s implementa√ß√£o bem-sucedida:

1. [ ] Considerar adicionar op√ß√£o de "velocidade" do streaming
2. [ ] Implementar cache de chunks (para regenera√ß√£o parcial)
3. [ ] Adicionar op√ß√£o de "pausar" gera√ß√£o
4. [ ] Permitir edi√ß√£o em tempo real durante gera√ß√£o
5. [ ] Analytics: medir tempo de gera√ß√£o vs satisfa√ß√£o do usu√°rio

---

**Boa implementa√ß√£o! üöÄ**
